# 深度学习之图像分类学习记录

## 一、numpy基础

 numpy数据结构是n维的数组对象，叫做ndarray。Python的list虽然也能表示，但是不高效，随着列表数据的增加，效率会降低。其中，切片操作及其相关符号非常重要：

":"用以表示当前维度的所有子模块；

"-1"用以表示当前维度所有子模块最后一个，"负号用以表示从后往前数的元素,-n即是表示从后往前数的第n个元素"；

：：的含义——start : end : step；

常用函数大全：https://blog.csdn.net/u011995719/article/details/71080987

### 1.一维数组处理

```python
#导入包并重命名
import numpy as np

#定义一维数组
a = np.array([2, 0, 1, 5, 8, 3])
print u'原始数据:', a

#输出最大、最小值及形状
print u'最小值:', a.min()
print u'最大值:', a.max()
print u'形状', a.shape

#数据切片
print u'切片操作:'
print a[:-2]
print a[-2:]
print a[:1]

#排序
print type(a)
a.sort()
print u'排序后:', a
```

输出：

```python
原始数据: [2 0 1 5 8 3]
最小值: 0
最大值: 8
形状 (6L,)
切片操作:
[2 0 1 5]
[8 3]
[2]
<type 'numpy.ndarray'>
排序后: [0 1 2 3 5 8]
```

注释：
1.代码通过np.array定义了一个数组[2, 0, 1, 5, 8, 3]，其中min计算最小值，max计算最大值，shape表示数组的形状，因为是一维数组，故6L（6个数字）。

2.最重要的一个知识点是数组的切片操作，因为在数据分析过程中，通常会对数据集进行"80%-20%"或"70%-30%"的训练集和测试集划分，通常采用的方法就是切片。
(1)a[:-2]表示从头开始获取，"-2"表示后面两个值不取，结果：[2 0 1 5]
(2)a[-2:]表示后往前数两个数字，获取数字至结尾，即获取最后两个值[8 3]
(3)a[:1]表示从头开始获取，获取1个数字，即[2]

### 2.二维数组处理

``` python
#定义二维数组
import numpy as np
c = np.array([[1, 2, 3, 4],[4, 5, 6, 7], [7, 8, 9, 10]])
 
#获取值
print u'形状:', c.shape
print u'获取值:', c[1][0]
print u'获取某行:'
print c[1][:]
print u'获取某行并切片:'
print c[0][:-1]
print c[0][-1:]
 
#获取具体某列值
print u'获取第3列:'
print c[:,np.newaxis, 2]
 
#调用sin函数
print np.sin(np.pi/6)
print type(np.sin(0.5))
 
#范围定义
print np.arange(0,4)
print type(np.arange(0,4))
```

输出结果：

``` python
形状: (3L, 4L)
获取值: 4
获取某行:
[4 5 6 7]
获取某行并切片:
[1 2 3]
[4]
获取第3列:
[[3]
 [6]
 [9]]
0.5
<type 'numpy.float64'>
[0 1 2 3]
<type 'numpy.ndarray'>
```

注释：

1.定义二维数组括号不要弄错，正确的应该是：[[1,2,3],[4,5,6]], 同时计算机的存储下标都是从0开始计算的。

2.获取二维数组中的某行，如第2行数据[4,5,6,7]，采用方法是:c[1] [:]。

3.获取二维数组中的某列，如第3列数据[[3] [6] [9]]，c[:,np.newaxis, 2]。因为通常在数据可视化中采用获取某列数据作为x或y坐标，同时多维数据也可以采用PCA降低成两维数据，再进行显示

## 二、pandas基础

Pandas是面板数据（Panel Data）的简写。它是Python最强大的数据分析和探索工具，因金融数据分析工具而开发，支持类似SQL的数据增删改查，支持时间序列分析，灵活处理缺失数据。它有两个主要的数据结构，Series和DataFrame，记住大小写区分。其中，相关知识需要了解：

data.describe()输出的相关信息——count：总数；mean：均值；std：标准差；min：最小值；25%：四分之一分位数；50%：中位数；75%：四分之三分位数；max：最大值；

数据提取用到三个函数loc,iloc和ix，loc函数按标签值进行提取，iloc按位置进行提取，ix可以同时按标签和位置进行提取。 

在pandas中用函数 isnull 和 notnull 来检测数据丢失：pd.isnull(a)、pd.notnull(b)，Series也提供了这些函数的实例方法：a.isnull()；

Pandas提供了大量的方法能够轻松的对Series，DataFrame和Panel对象进行各种符合各种逻辑关系的合并操作。如：Concat、Merge （类似于SQL类型的合并）、Append （将一行连接到一个DataFrame上）；

常用函数大全：https://blog.csdn.net/liufang0001/article/details/77856255



### 1.读写文件

对于电力用户数据集missing_data.xls文件，内容如下，共3列数据，分别是用户A、用户B、用户C，共21行，对应21天的用电量，其中包含缺失值。

```python
235.8333	324.0343	478.3231
236.2708	325.6379	515.4564
238.0521	328.0897	517.0909
235.9063		        514.89
236.7604	268.8324	
	        404.048     486.0912
237.4167	391.2652	516.233
238.6563	380.8241	
237.6042	388.023	    435.3508
238.0313	206.4349	487.675
235.0729		
235.5313	400.0787	660.2347
	        411.2069	621.2346
234.4688	395.2343	611.3408
235.5	    344.8221	643.0863
235.6354	385.6432	642.3482
234.5521	401.6234	
236	        409.6489	602.9347
235.2396	416.8795	589.3457
235.4896		        556.3452
236.9688		        538.347
```

输入：

```python
#读取数据 header设置Excel无标题头
import pandas as pd
data = pd.read_excel("missing_data.xls", header=None) 
print data
 
#计算数据长度
print u'行数', len(data)
 
#计算用户A\B\C用电总和
print data.sum()
 
#计算用户A\B\C用点量算术平均数
mm = data.sum()
print mm
 
#输出预览前5行数据
print u'预览前5行数据'
print data.head()
 
#输出数据基本统计量
print u'输出数据基本统计量'
print data.describe()
```

输出：

```python
           0         1         2
0   235.8333  324.0343  478.3231
1   236.2708  325.6379  515.4564
2   238.0521  328.0897  517.0909
3   235.9063       NaN  514.8900
4   236.7604  268.8324       NaN
5        NaN  404.0480  486.0912
6   237.4167  391.2652  516.2330
7   238.6563  380.8241       NaN
8   237.6042  388.0230  435.3508
...
行数 21
0    4488.9899
1    6182.3265
2    9416.3276
dtype: float64
0    4488.9899
1    6182.3265
2    9416.3276
dtype: float64
预览前5行数据
          0         1         2
0  235.8333  324.0343  478.3231
1  236.2708  325.6379  515.4564
2  238.0521  328.0897  517.0909
3  235.9063       NaN  514.8900
4  236.7604  268.8324       NaN
输出数据基本统计量
                0           1           2
count   19.000000   17.000000   17.000000
mean   236.262626  363.666265  553.901624
std      1.225465   57.600529   67.707729
min    234.468800  206.434900  435.350800
25%           NaN         NaN         NaN
50%           NaN         NaN         NaN
75%           NaN         NaN         NaN
max    238.656300  416.879500  660.234700
```

注释：

1.因为Excel表格中存在空值，故Python显示为NaN（Not a Number）表示空。

### 2.Series

 Series是一维标记数组，可以存储任意数据类型，如整型、字符串、浮点型和Python对象等，轴标一般指索引。
Series、Numpy中的一维array 、Python基本数据结构List区别：List中的元素可以是不同的数据类型，而Array和Series中则只允许存储相同的数据类型，这样可以更有效的使用内存，提高运算效率。

```python
from pandas import Series, DataFrame
 
#通过传递一个list对象来创建Series，默认创建整型索引；
a = Series([4, 7, -5, 3])
print u'创建Series:'
print a
 
#创建一个带有索引来确定每一个数据点的Series 
b = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
print u'创建带有索引的Series:'
print b
 
#如果你有一些数据在一个Python字典中，你可以通过传递字典来创建一个Series
sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}
c = Series(sdata)
print u'通过传递字典创建Series:'
print c
states = ['California', 'Ohio', 'Oregon', 'Texas']
d = Series(sdata, index=states)
print u'California没有字典为空:'
print d
```

输出：

```python
创建Series:
0    4
1    7
2   -5
3    3
dtype: int64
创建带有索引的Series:
d    4
b    7
a   -5
c    3
dtype: int64
通过传递字典创建Series:
Ohio      35000
Oregon    16000
Texas     71000
Utah       5000
dtype: int64
California没有字典为空:
California        NaN
Ohio          35000.0
Oregon        16000.0
Texas         71000.0
dtype: float64
```

注释：

1.Series的一个重要功能是在算术运算中它会自动对齐不同索引的数据，示例中当自定义的索引California 和字典队员不上时，会自动选择NaN，即结果为空，表示缺失。

### 3.DataFrame

 DataFrame是二维标记数据结构，列可以是不同的数据类型。它是最常用的pandas对象，像Series一样可以接收多种输入：lists、dicts、series和DataFrame等。初始化对象时，除了数据还可以传index和columns这两个参数。类似一张excel表格或者SQL，只是功能更强大。构建DataFrame的方法有很多，最常用的是传入一个字典。

```python
df = pd.DataFrame({"id":[1001,1002,1003,1004,1005,1006], 
 "date":pd.date_range('20130102', periods=6),
  "city":['Beijing ', 'SH', ' guangzhou ', 'Shenzhen', 'shanghai', 'BEIJING '],
 "age":[23,44,54,32,34,32],
 "category":['100-A','100-B','110-A','110-C','210-A','130-F'],
  "price":[1200,np.nan,2133,5433,np.nan,4432]},
  columns =['id','date','city','category','age','price'])
df.info()

```

输出

```python
RangeIndex: 6 entries, 0 to 5
Data columns (total 6 columns):
id          6 non-null int64
date        6 non-null datetime64[ns]
city        6 non-null object
category    6 non-null object
age         6 non-null int64
price       4 non-null float64
dtypes: datetime64[ns](1), float64(1), int64(2), object(2)
memory usage: 368.0+ bytes
```

注释：
1.DataFrame中常常会出现重复行，DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行；还有一个drop_duplicated方法，它返回一个移除了重复行的DataFrame。

## 二、示例代码调试

### 1.mnist_cnn.py

基于MNIST数据集的卷积神经网络。使用CNN对MNIST数据集（包含7千张28*28的单通道（灰度图、黑白图）图片）分类（0-9，10个类别）。mnist为手写数据集。MNIST（Mixed National Institute of Standards and Technology database）是一个计算机视觉数据集，它包含70000张手写数字的灰度图片，其中每一张图片包含 28 X 28 个像素点。

```python
'''Trains a simple convnet on the MNIST dataset.
Gets to 99.25% test accuracy after 12 epochs
(there is still a lot of margin for parameter tuning).
16 seconds per epoch on a GRID K520 GPU.
'''
##训练一个基于MNIST数据集的简单卷积神经网络
##12个周期后达到99.25%的精确度，通过参数调整还可提升精确度
##使用一个GRID K520 GPU （图形处理器）每个周期16秒
from __future__ import print_function
#Python提供的__future__模块把下一个新版本的特性导入到当前版本
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K

batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
```



输出：

```python

runfile('C:/Users/54228/.spyder-py3/temp.py', wdir='C:/Users/54228/.spyder-py3')
Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz
11493376/11490434 [==============================] - 777s 68us/step
WARNING: Logging before flag parsing goes to stderr.
W0804 01:10:42.718356 25216 deprecation_wrapper.py:119] From D:\app\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
W0804 01:10:43.178099 25216 deprecation_wrapper.py:119] From D:\app\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0804 01:10:43.290821 25216 deprecation_wrapper.py:119] From D:\app\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

W0804 01:10:43.413498 25216 deprecation_wrapper.py:119] From D:\app\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

W0804 01:10:43.416486 25216 deprecation_wrapper.py:119] From D:\app\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

W0804 01:10:43.437407 25216 deprecation.py:506] From D:\app\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
W0804 01:10:43.526170 25216 deprecation_wrapper.py:119] From D:\app\Anaconda3\lib\site-packages\keras\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0804 01:10:43.533183 25216 deprecation_wrapper.py:119] From D:\app\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.

W0804 01:10:43.858312 25216 deprecation.py:323] From D:\app\Anaconda3\lib\site-packages\tensorflow\python\ops\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 112s 2ms/step - loss: 0.2627 - acc: 0.9197 - val_loss: 0.0621 - val_acc: 0.9803
Epoch 2/12
60000/60000 [==============================] - 120s 2ms/step - loss: 0.0922 - acc: 0.9726 - val_loss: 0.0401 - val_acc: 0.9864
Epoch 3/12
60000/60000 [==============================] - 99s 2ms/step - loss: 0.0676 - acc: 0.9792 - val_loss: 0.0397 - val_acc: 0.9869
Epoch 4/12
60000/60000 [==============================] - 104s 2ms/step - loss: 0.0550 - acc: 0.9839 - val_loss: 0.0320 - val_acc: 0.9893
Epoch 5/12
60000/60000 [==============================] - 110s 2ms/step - loss: 0.0478 - acc: 0.9860 - val_loss: 0.0310 - val_acc: 0.9894
Epoch 6/12
60000/60000 [==============================] - 110s 2ms/step - loss: 0.0423 - acc: 0.9872 - val_loss: 0.0281 - val_acc: 0.9906
Epoch 7/12
60000/60000 [==============================] - 115s 2ms/step - loss: 0.0391 - acc: 0.9882 - val_loss: 0.0300 - val_acc: 0.9899
Epoch 8/12
60000/60000 [==============================] - 86s 1ms/step - loss: 0.0357 - acc: 0.9892 - val_loss: 0.0271 - val_acc: 0.9910
Epoch 9/12
60000/60000 [==============================] - 83s 1ms/step - loss: 0.0332 - acc: 0.9903 - val_loss: 0.0289 - val_acc: 0.9904
Epoch 10/12
60000/60000 [==============================] - 79s 1ms/step - loss: 0.0315 - acc: 0.9900 - val_loss: 0.0274 - val_acc: 0.9911
Epoch 11/12
60000/60000 [==============================] - 80s 1ms/step - loss: 0.0281 - acc: 0.9915 - val_loss: 0.0282 - val_acc: 0.9919
Epoch 12/12
60000/60000 [==============================] - 83s 1ms/step - loss: 0.0281 - acc: 0.9917 - val_loss: 0.0309 - val_acc: 0.9908
Test loss: 0.030856597457616316
Test accuracy: 0.9908
```

### 2.MNIST in Keras.ipynb

```python
 %matplotlib inline
```

#### Import some prerequisites

```python
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.utils import np_utils
```

#### Load training data

```python
nb_classes = 10

# the data, shuffled and split between tran and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()
print("X_train original shape", X_train.shape)
print("y_train original shape", y_train.shape)

for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(X_train[i], cmap='gray', interpolation='none')
    plt.title("Class {}".format(y_train[i]))
```

输出

```python
X_train original shape (60000, 28, 28)
y_train original shape (60000,)
```

![img](https://raw.githubusercontent.com/zhangyf99/Deeplearning/master/static/1-zyf.png)

#### Format the data for training

```python
X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
```

输出：

```python
Training matrix shape (60000, 784)
Testing matrix shape (10000, 784)
```

#### Build the neural network

```python
model = Sequential()
model.add(Dense(512, input_shape=(784,)))
model.add(Activation('relu')) # An "activation" is just a non-linear function applied to the output
                              # of the layer above. Here, with a "rectified linear unit",
                              # we clamp all values below 0 to 0.
                           
model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or "overfitting" the training data
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(10))
model.add(Activation('softmax')) # This special "softmax" activation among other things,
                                 # ensures the output is a valid probaility distribution, that is
                                 # that its values are all non-negative and sum to 1.
```



#### Compile the model

```python
model.compile(loss='categorical_crossentropy', optimizer='adam')
```

#### Train the model

```python
model.fit(X_train, Y_train,
          batch_size=128, nb_epoch=4,
          verbose=1,
          validation_data=(X_test, Y_test))
```

输出：

```python
Train on 60000 samples, validate on 10000 samples
Epoch 1/4
60000/60000 [==============================] - 15s 255us/step - loss: 0.2481 - val_loss: 0.1120
Epoch 2/4
60000/60000 [==============================] - 13s 216us/step - loss: 0.1022 - val_loss: 0.0829
Epoch 3/4
60000/60000 [==============================] - 13s 220us/step - loss: 0.0708 - val_loss: 0.0625
Epoch 4/4
60000/60000 [==============================] - 12s 193us/step - loss: 0.0558 - val_loss: 0.0662
Out[10]:
<keras.callbacks.History at 0x1ccc53690b8>
```



#### Evaluate its performance

```python
score = model.evaluate(X_test, Y_test,
                     verbose=0)
##print('Test score:', score[0])
##print('Test accuracy:', score[1]
print('Test score:', score)

# The predict_classes function outputs the highest probability class
# according to the trained classifier for each input example.
predicted_classes = model.predict_classes(X_test)

# Check which items we got right / wrong
correct_indices = np.nonzero(predicted_classes == y_test)[0]
incorrect_indices = np.nonzero(predicted_classes != y_test)[0]

plt.figure()
for i, correct in enumerate(correct_indices[:9]):
    plt.subplot(3,3,i+1)
    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')
    plt.title("Predicted {}, Class {}".format(predicted_classes[correct], y_test[correct]))
    
plt.figure()
for i, incorrect in enumerate(incorrect_indices[:9]):
    plt.subplot(3,3,i+1)
    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
    plt.title("Predicted {}, Class {}".format(predicted_classes[incorrect], y_test[incorrect]))
```

输出：

```python
Test score: 0.0662498661814956
```

![img](https://raw.githubusercontent.com/zhangyf99/Deeplearning/master/static/2-zyf.png)

![img](https://raw.githubusercontent.com/zhangyf99/Deeplearning/master/static/3-zyf.png)

